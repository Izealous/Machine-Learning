{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5fbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d885f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis(object):\n",
    "    \"\"\"Class to get sentiment score based on analyzer.\"\"\"\n",
    "\n",
    "    def __init__(self, filename='SentiWordNet.txt', weighting='geometric'):\n",
    "        \"\"\"Initialize with filename and choice of weighting.\"\"\"\n",
    "        if weighting not in ('geometric', 'harmonic', 'average'):\n",
    "            raise ValueError(\n",
    "                'Allowed weighting options are geometric, harmonic, average')\n",
    "        # parse file and build sentiwordnet dicts\n",
    "        self.swn_pos = {'a': {}, 'v': {}, 'r': {}, 'n': {}}\n",
    "        self.swn_all = {}\n",
    "        self.build_swn(filename, weighting)\n",
    "\n",
    "    def average(self, score_list):\n",
    "        \"\"\"Get arithmetic average of scores.\"\"\"\n",
    "        if(score_list):\n",
    "            return sum(score_list) / float(len(score_list))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def geometric_weighted(self, score_list):\n",
    "        \"\"\"\"Get geometric weighted sum of scores.\"\"\"\n",
    "        weighted_sum = 0\n",
    "        num = 1\n",
    "        for el in score_list:\n",
    "            weighted_sum += (el * (1 / float(2**num)))\n",
    "            num += 1\n",
    "        return weighted_sum\n",
    "\n",
    "    # another possible weighting instead of average\n",
    "    def harmonic_weighted(self, score_list):\n",
    "        \"\"\"Get harmonic weighted sum of scores.\"\"\"\n",
    "        weighted_sum = 0\n",
    "        num = 2\n",
    "        for el in score_list:\n",
    "            weighted_sum += (el * (1 / float(num)))\n",
    "            num += 1\n",
    "        return weighted_sum\n",
    "\n",
    "    def build_swn(self, filename, weighting):\n",
    "        \"\"\"Build class's lookup based on SentiWordNet 3.0.\"\"\"\n",
    "        records = [line.split('\\t') for line in open(filename)]\n",
    "        for rec in records:\n",
    "            # has many words in 1 entry\n",
    "            words = rec[4].split()\n",
    "            pos = rec[0]\n",
    "            for word_num in words:\n",
    "                word = word_num.split('#')[0]\n",
    "                sense_num = int(word_num.split('#')[1])\n",
    "\n",
    "                # build a dictionary key'ed by sense number\n",
    "                if word not in self.swn_pos[pos]:\n",
    "                    self.swn_pos[pos][word] = {}\n",
    "                self.swn_pos[pos][word][sense_num] = float(\n",
    "                    rec[2]) - float(rec[3])\n",
    "                if word not in self.swn_all:\n",
    "                    self.swn_all[word] = {}\n",
    "                self.swn_all[word][sense_num] = float(rec[2]) - float(rec[3])\n",
    "\n",
    "        # convert innermost dicts to ordered lists of scores\n",
    "        for pos in self.swn_pos.keys():\n",
    "            for word in self.swn_pos[pos].keys():\n",
    "                newlist = [self.swn_pos[pos][word][k] for k in sorted(\n",
    "                    self.swn_pos[pos][word].keys())]\n",
    "                if weighting == 'average':\n",
    "                    self.swn_pos[pos][word] = self.average(newlist)\n",
    "                if weighting == 'geometric':\n",
    "                    self.swn_pos[pos][word] = self.geometric_weighted(newlist)\n",
    "                if weighting == 'harmonic':\n",
    "                    self.swn_pos[pos][word] = self.harmonic_weighted(newlist)\n",
    "\n",
    "        for word in self.swn_all.keys():\n",
    "            newlist = [self.swn_all[word][k] for k in sorted(\n",
    "                self.swn_all[word].keys())]\n",
    "            if weighting == 'average':\n",
    "                self.swn_all[word] = self.average(newlist)\n",
    "            if weighting == 'geometric':\n",
    "                self.swn_all[word] = self.geometric_weighted(newlist)\n",
    "            if weighting == 'harmonic':\n",
    "                self.swn_all[word] = self.harmonic_weighted(newlist)\n",
    "\n",
    "    def pos_short(self, pos):\n",
    "        \"\"\"Convert NLTK POS tags to SWN's POS tags.\"\"\"\n",
    "        if pos in set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "            return 'v'\n",
    "        elif pos in set(['JJ', 'JJR', 'JJS']):\n",
    "            return 'a'\n",
    "        elif pos in set(['RB', 'RBR', 'RBS']):\n",
    "            return 'r'\n",
    "        elif pos in set(['NNS', 'NN', 'NNP', 'NNPS']):\n",
    "            return 'n'\n",
    "        else:\n",
    "            return 'a'\n",
    "\n",
    "    def score_word(self, word, pos):\n",
    "        \"\"\"Get sentiment score of word based on SWN and part of speech.\"\"\"\n",
    "        try:\n",
    "            return self.swn_pos[pos][word]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                return self.swn_all[word]\n",
    "            except KeyError:\n",
    "                return 0\n",
    "\n",
    "    def score(self, sentence):\n",
    "        \"\"\"Sentiment score a sentence.\"\"\"\n",
    "        # init sentiwordnet lookup/scoring tools\n",
    "        impt = set(['NNS', 'NN', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS',\n",
    "                    'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN',\n",
    "                    'VBP', 'VBZ', 'unknown'])\n",
    "        non_base = set(['VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'NNS', 'NNPS'])\n",
    "        negations = set(['not', 'n\\'t', 'less', 'no', 'never',\n",
    "                         'nothing', 'nowhere', 'hardly', 'barely',\n",
    "                         'scarcely', 'nobody', 'none'])\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "        scores = []\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "        index = 0\n",
    "        for el in tagged:\n",
    "\n",
    "            pos = el[1]\n",
    "            try:\n",
    "                word = re.match('(\\w+)', el[0]).group(0).lower()\n",
    "                start = index - 5\n",
    "                if start < 0:\n",
    "                    start = 0\n",
    "                neighborhood = tokens[start:index]\n",
    "\n",
    "                # look for trailing multiword expressions\n",
    "                word_minus_one = tokens[index-1:index+1]\n",
    "                word_minus_two = tokens[index-2:index+1]\n",
    "\n",
    "                # if multiword expression, fold to one expression\n",
    "                if(self.is_multiword(word_minus_two)):\n",
    "                    if len(scores) > 1:\n",
    "                        scores.pop()\n",
    "                        scores.pop()\n",
    "                    if len(neighborhood) > 1:\n",
    "                        neighborhood.pop()\n",
    "                        neighborhood.pop()\n",
    "                    word = '_'.join(word_minus_two)\n",
    "                    pos = 'unknown'\n",
    "\n",
    "                elif(self.is_multiword(word_minus_one)):\n",
    "                    if len(scores) > 0:\n",
    "                        scores.pop()\n",
    "                    if len(neighborhood) > 0:\n",
    "                        neighborhood.pop()\n",
    "                    word = '_'.join(word_minus_one)\n",
    "                    pos = 'unknown'\n",
    "\n",
    "                # perform lookup\n",
    "                if (pos in impt) and (word not in stopwords):\n",
    "                    if pos in non_base:\n",
    "                        word = wnl.lemmatize(word, self.pos_short(pos))\n",
    "                    score = self.score_word(word, self.pos_short(pos))\n",
    "                    if len(negations.intersection(set(neighborhood))) > 0:\n",
    "                        score = -score\n",
    "                    scores.append(score)\n",
    "\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        if len(scores) > 0:\n",
    "            return sum(scores) / float(len(scores))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def is_multiword(self, words):\n",
    "        \"\"\"Test if a group of words is a multiword expression.\"\"\"\n",
    "        joined = '_'.join(words)\n",
    "        return joined in self.swn_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89926d1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 3672-3673: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14768/2392496411.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hasilpreproces_en_bro.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SentiWordNet.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweighting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'geometric'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 3672-3673: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('hasilpreproces_en_bro.csv')\n",
    "s = SentimentAnalysis(filename='SentiWordNet.txt',weighting='geometric')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05067e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_sentiword = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    sentiword = s.score(row['text'])\n",
    "    array_sentiword.append(sentiword)\n",
    "    \n",
    "df = pd.DataFrame(array_sentiword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e7d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"hasil_sentiwordnet_bro.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843958bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.110596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.050347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.030322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11533</th>\n",
       "      <td>0.006165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11534</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11535</th>\n",
       "      <td>0.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>-0.028996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>0.013672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11538 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0      0.000000\n",
       "1      0.110596\n",
       "2      0.061198\n",
       "3     -0.050347\n",
       "4      0.030322\n",
       "...         ...\n",
       "11533  0.006165\n",
       "11534  0.000000\n",
       "11535  0.073100\n",
       "11536 -0.028996\n",
       "11537  0.013672\n",
       "\n",
       "[11538 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa36f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
